\chapter{Results}
In this chapter, we take a look at results of our classification. The whole pipeline is run on the Salomon supercomputer at IT4Innovations \cite{Salomon-WWW-17}. Each task was computed on a single compute node. Each compute node contains two 2.5 GHz, 12-core Intel Xeon E5-2680v3 (Haswell) processors and 128 GB of memory.

\section{Tools}
The data preprocessing and feature extraction of the Cats and Dogs dataset are done using the tools-iiit-pet from our ml4py toolkit \cite{tools_iiit}. The tool for the data preparation and feature extraction of the other two datasets is strongly inspired by the tools-iiit-pet tool.

For the local features extraction, we use the implementation of SIFT, SURF, and ORB from the OpenCV library \cite{opencv} and $k$-means implementation from the SciPy library \cite{scipy}. The BoVW is created in Python.

The global feature extractor, PCA, is implemented using PETSc \cite{petsc} and SLEPc \cite{slepc} libraries.

The Bayesian Model classifier using the Jensen inequality is implemented in python. The Bayesian Model solved using the Spectral Projected Gradient method is trained using an Octave code provided by Ing. Lukáš Pospíšil, Ph.D., while the testing is done in python.

For the SVM classification, we use the PermonSVM \cite{permonSVM}. This implementation of the SVM algorithm comes from the Department of Applied Mathematics, VSB-TUO, and the Institute of Geonics of the Czech Academy of Science in Ostrava, Czech Republic. It is build on top of a package for large scale quadratic programming, PermonQP. Both, PermonSVM and PermonQP, make use of the PETSc \cite{petsc} library.

\section{2D Shapes Dataset Classification}
In the 2D Shapes classification, we attempt to train and classify images of a circle, labeled as the positive data, and images of a star, labeled as the negative data. See section \ref{sec:2d-dataset} for details of benchmark formulation.

For local extractors, we examine the classification of BoVW with dictionary sizes $2, 3, 4, 6, 8, 12, 16$.

\subsection{SVM}

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_svm_PCA.tex}
    \caption[2D Shapes result for PCA extraction and SVM classification]{2D Shapes result for PCA extraction and SVM classification. \say{Ext} is shorthand for \say{Extraction}.}
    \label{tab:2d_PCA_SVM}
\end{table}
In \tabref{tab:2d_PCA_SVM}, we present results of the classification by the SVM model on data extracted using PCA. We keep only the first $n$ eigenvectors, which can together explain at least $95\%$ of the data variance. For the 2D Shapes dataset, we keep the first $12$ eigenvectors.

Both, the $l_1$ and the $l_2$ SVM, classified the testing data quite well as we can see from the score. However, by the number of itterations, we can see that the $l_1$-loss SVM model was unable to converge in less than $10,000$ itterations, and therefore it was stopped early.

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_svm_SIFT.tex}
    \caption[2D Shapes results for extraction: SIFT and classification: SVM]{2D Shapes results for extraction: SIFT and classification: SVM.}
    \label{tab:2d_SIFT_SVM}
\end{table}
In \tabref{tab:2d_SIFT_SVM}, we can see results of the classification by the SVM model on data extracted using SIFT. Since the data is quite simple, the classifier has no problem classifying the data for any size of the BoVW dictionary. However, we can see, that both, the SVM-l1 and SVM-l2, converged much faster for larger sizes of the BoVW dictionary. Moreover, we recieved perfect score for accuracy, precission, and $F_1$ from the BoVW sizes beyond the size of $6$.

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_svm_SURF.tex}
    \caption[2D Shapes results for extraction: SURF and classification: SVM]{2D Shapes results for extraction: SURF and classification: SVM.}
    \label{tab:2d_SURF_SVM}
\end{table}
In \tabref{tab:2d_SURF_SVM}, we can see results of the classification by the SVM model on data extracted using SURF. Here, we can note, that for the size of BoVW dictionary $3$, the classification turned out poorly. This might have been due to poorly located centroids by the $k$-means algorithm. However, we can see that for other sizes of BoVW dictionary, we get satisfactory, while mostly worse than in the case of extraction using SIFT, results.

Compared to SIFT, the extraction of SURF features runs much quicker. Even though the classification was in most cases slower, the time saved in extraction more than made up for it.

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_svm_ORB.tex}
    \caption[2D Shapes results for extraction: ORB and classification: SVM]{2D Shapes results for extraction: ORB and classification: SVM.}
    \label{tab:2d_ORB_SVM}
\end{table}
In \tabref{tab:2d_ORB_SVM}, we can see results of the classification by the SVM model on data extracted using ORB. In this case, the the classification turned out better than in the case of SURF, while the time complexity did not worsen by much.

\subsection{Bayesian Model Classification}
\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_bayes_SIFT.tex}
    \caption[2D Shapes results for SIFT extraction and Bayesian model classification]{2D Shapes results for SIFT extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:2d_SIFT_bayes}
\end{table}
In \tabref{tab:2d_ORB_SVM}, we can see results of the classification by the Bayesian model on data extracted using SIFT. In contrast with the classification using the SVM, we can see that the Bayesian model classifier has trouble classifying the data for smallest BoVW dictionary sizes. However, starting at the BoVW size $6$, we recieved perfect or near perfect score for accuracy, precision, and $F_1$ with both models.

Looking at the training duration, we can see the advantage of the analytical solution using Jensen inequality compared to the numerical solution using Spectral Projected Gradient method. This trend continues across the different extraction techniques.

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_bayes_SURF.tex}
    \caption[2D Shapes results for SURF extraction and Bayesian model classification]{2D Shapes results for SURF extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:2d_SURF_bayes}
\end{table}
In \tabref{tab:2d_ORB_SVM}, we can see results of the classification by the Bayesian model on data extracted using SURF. With SURF, the Bayesian model using Jensen inequality performed well for all sizes of the BoVW dictionary. However, the Bayesian model using Spectral Projected Gradient performed quite poorly with the BoVW dictionary size of $8$. We again suspect the reason to be poorly located centroids by the $k$-means algorithm.

\begin{table}[ht!]
    \centering
    \input{tables/four-shapes_bayes_ORB.tex}
    \caption[2D Shapes results for ORB extraction and Bayesian model classification]{2D Shapes results for ORB extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:2d_ORB_bayes}
\end{table}
In \tabref{tab:2d_ORB_SVM}, we can see results of the classification by the Bayesian model on data extracted using ORB. It performed better than the classification of the extracted features using SIFT for the size $2$ of the BoVW dictionary. Moreover, we can got a perfect classification for all the other sizes of the BoVW dictionary.

For our simple 2D Shapes dataset, the best classification is achieved using the analytical solution of the Bayesian model on the data extracted using ORB, with the BoVW dictionary size $3$ or more.

From the standpoint of time coplexity, the analytical solution of the Bayesian model shows clear advantage over the other methods. Since the SVM classifier was trained using parallelization on $24$ cores, while both Bayesian models were run in linear, the advantage is even more substentional.

\section{3D Shapes Dataset Classification}
In the 3D Shapes classification, we move on to more complex images. We attempt classify images of a box, labeled as the positive data, and images of a sphere, labeled as the negative data. This dataset has much less data than the other two (only $20$ images of each class). See section \ref{sec:3d-dataset} for details of benchmark formulation.

Same as with the 2D shapes, for local extractors, we attempt the classification of BoVW with dictionary sizes $2, 3, 4, 6, 8, 12, 16$.

\subsection{SVM}
\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_svm_PCA.tex}
    \caption[3D Shapes result for PCA extraction and SVM classification]{3D Shapes result for PCA extraction and SVM classification. \say{Ext} is shorthand for \say{Extraction}.}
    \label{tab:3d_PCA_SVM}
\end{table}
In \tabref{tab:3d_PCA_SVM}, we present results of the classification by the SVM model on data extracted using PCA. For the 3D Shapes dataset, we keep the first $11$ largest eigenvectors, which can explain at least $95\%$ of the variance.

The training of both the SVM-$l_1$ and the SVM-$l_2$ converged in alloted number of itteration. However, the training of the $l_1$ classification model took magnitude more itteration steps. The classification score for both is acceptable.

\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_svm_SIFT.tex}
    \caption[3D Shapes results for extraction: SIFT and classification: SVM]{3D Shapes results for extraction: SIFT and classification: SVM.}
    \label{tab:3d_SIFT_SVM}
\end{table}
In \tabref{tab:3d_SIFT_SVM}, we can see results of the classification by the SVM model on data extracted using SIFT. We achieved satisfactory results from classification of the BoVW with a dictionary size of $4$ and above. Compared to the 2D dataset, we need bigger dictionary size, which can be explained by the increase in complexity of the data.

\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_svm_SURF.tex}
    \caption[3D Shapes results for extraction: SURF and classification: SVM]{3D Shapes results for extraction: SURF and classification: SVM.}
    \label{tab:3d_SURF_SVM}
\end{table}
In \tabref{tab:3d_SURF_SVM}, we can see results of the classification by the SVM model on data extracted using SURF. Here, both the $l1$-loss and the $l2$-loss SVM classifiers have trouble classifying the data for the size of BoVW dictionary smaller than $6$. The \say{nan} of $F_1$ score for the size of BoVW $2$ are due to none of the positive data (boxes) being classified as such. For the sizes of BoVW $2$, $3$, and $4$, we came across technical difficulties with the hyperoptimization of the $l_1$ model. Same technical difficulties were encountered for the BoVW of size $2$ in the case of $l_2$ model. As the hyperoptimization was not performed for these cases, we note the duration of the hyperoptimization as \say{nan}.

\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_svm_ORB.tex}
    \caption[3D Shapes results for extraction: ORB and classification: SVM]{3D Shapes results for extraction: ORB and classification: SVM.}
    \label{tab:3d_ORB_SVM}
\end{table}
In \tabref{tab:3d_ORB_SVM}, we can see results of the classification by the SVM model on data extracted using ORB. Here, we can see even worse results than classifying the data extracted using SURF, as the classifiers struggless with BoVW sizes smaller than $8$.

For both, the SURF and ORB extracted data classification, the time complexity improvement is not worth the worse classification score.

\subsection{Bayesian Model Classificatin}
\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_bayes_SIFT.tex}
    \caption[3D Shapes results for SIFT extraction and Bayesian model classification]{3D Shapes results for SIFT extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:3d_SIFT_bayes}
\end{table}
In \tabref{tab:3d_SIFT_bayes}, we present results of the classification by the Bayesian model on data extracted using SIFT. We can see, similar to classification using SVM, that for the BoVW size of $4$ or more, the Bayesian classifier using Jensen inequality gives perfect results. However, the classifier using Spectral Projected Gradient method does not classify well for the BoVW of the size $6$.

\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_bayes_SURF.tex}
    \caption[3D Shapes results for SURF extraction and Bayesian model classification]{3D Shapes results for SURF extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:3d_SURF_bayes}
\end{table}
\begin{table}[ht!]
    \centering
    \input{tables/3d_shapes_bayes_ORB.tex}
    \caption[3D Shapes results for ORB extraction and Bayesian model classification]{3D Shapes results for ORB extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:3d_ORB_bayes}
\end{table}
In \tabref{tab:3d_SURF_bayes} and \tabref{tab:3d_ORB_bayes} we can see results of the classification by the Bayesian model on data extracted using SURF and ORB respectively. Similar to the classification using SVM, we can see that the Bayes classifier struggles with the classification until larger sizes of BoVW.

On the 3D Shapes dataset, we can see an advantage of classifying data extracted using the SIFT extractor. While the SIFT extractor is slower than the SURF and the ORB extractors, the performance of the classification model is much better.

\section{Cats and Dogs Dataset Classification}
In the Cats and Dogs classification, we attempt to classify images of real photographs. The complexity of the data is much bigger than the previous two datasets. To concentrate only on classification using the data relevant to the animal, we classify images of cutouts of the animals as described in section \ref{sec:cat_dog_dataset}.

\subsection{SVM}
In \tabref{tab:iiit_PCA_SVM}, we present results of the classification by the SVM model on data extracted using PCA. For the 3D Shapes dataset, we keep the first $81$ largest eigenvectors, which can explain at least $95\%$ of the variance.
\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_svm_PCA.tex}
    \caption[Cats and Dogs result for PCA extraction and SVM classification]{Cats and Dogs result for PCA extraction and SVM classification. \say{Ext} is shorthand for \say{Extraction}.}
    \label{tab:iiit_PCA_SVM}
\end{table}

Due to technical difficulties, we were unable to perform hyperoptimization for the $l_1$-SVM model. Moreover, the $l_1$ model did not manage to converge in the alloted $10,000$ itterations. While the $l_2$-SVM model converged, it was not useful for the testing data classification, which can be seen from the classification score.

\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_svm_SIFT.tex}
    \caption[Cats and Dogs results for extraction: SIFT and classification: SVM]{Cats and Dogs results for extraction: SIFT and classification: SVM.}
    \label{tab:iiit_SIFT_SVM}
\end{table}
In \tabref{tab:iiit_SIFT_SVM}, we can see results of the classification by the SVM model on data extracted using SIFT. Because of the complexity of the data, we get much worse results than in the cases of the 2D Shapes and 3D Shapes datasets. However, the classification model is better than a random classifier.

\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_svm_SURF.tex}
    \caption[Cats and Dogs results for extraction: SURF and classification: SVM]{Cats and Dogs results for extraction: SURF and classification: SVM.}
    \label{tab:iiit_SURF_SVM}
\end{table}
In \tabref{tab:iiit_SURF_SVM}, we can see results of the classification by the SVM model on data extracted using SURF.

\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_svm_ORB.tex}
    \caption[Cats and Dogs results for extraction: ORB and classification: SVM]{Cats and Dogs results for extraction: ORB and classification: SVM.}
    \label{tab:iiit_ORB_SVM}
\end{table}
In \tabref{tab:iiit_ORB_SVM}, we can see results of the classification by the SVM model on data extracted using ORB. The classifier behaves poorly in regard to precision and in regard to the $F_1$ score. In some cases, none of the data belonging to the positive class were classified as such, therefore value of the $F_1$ score cannot be calculated as it would lead to a division by zero. In the table, this is shown as a value \say{nan}.

\subsection{Bayesian Model Classificatin}
\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_bayes_SIFT.tex}
    \caption[Cats and Dogs results for SIFT extraction and Bayesian model classification]{Cats and Dogs results for SIFT extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:iiit_SIFT_bayes}
\end{table}
In \tabref{tab:iiit_SIFT_bayes}, we present results of the classification by the Bayesian model on data extracted using SIFT. The Bayesian model using Jensen inequality outperforms the SVM model in most cases. However, the Baesian model using Spectral Projected gradients has trouble with BoVW sizes greater than $8$ in regards to the $F_1$ score.

\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_bayes_SURF.tex}
    \caption[Cats and Dogs results for SURF extraction and Bayesian model classification]{Cats and Dogs results for SURF extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:iiit_SURF_bayes}
\end{table}
In \tabref{tab:iiit_SURF_bayes}, we present results of the classification by the Bayesian model on data extracted using SURF.

\begin{table}[ht!]
    \centering
    \input{tables/iiit-pet_bayes_ORB.tex}
    \caption[Cats and Dogs results for ORB extraction and Bayesian model classification]{Cats and Dogs results for ORB extraction and Bayesian model classification. \say{Acc} stands for accuracy and \say{Prec} stands for precision.}
    \label{tab:iiit_ORB_bayes}
\end{table}
In \tabref{tab:iiit_ORB_bayes}, we present results of the classification by the Bayesian model on data extracted using ORB.
