\section{Metrics}
To assess the quality of the classification model, we need to analyze several metrics. Many of such metrics are derived from a confusion matrix (see \tabref{tab:confusion_matrix}).
\begin{table}[ht]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        & Predicted negative & Predicted positive \\ 
        \hline 
        Actual negative & True negatives ($TN$) & False positives ($FP$) \\ 
        \hline
        Actual positive & False negatives ($FN$) & True positives ($TP$)  \\ 
        \hline
    \end{tabular}
    \caption{Confusion Matrix}
    \label{tab:confusion_matrix}
\end{table}
The confusion matrix is generated by counting the testing data, which are supposed to belong to a negative class (Actual negative) or a positive class (Actual positive), and their predicted class (Predicted negative/Predicted positive).

We are considering the following metrics in our experiments:
\begin{description}
    \item{\textbf{Accuracy:}} How often is the classifier correct overall. This metric is represented in percentages.
    \begin{equation}
        \frac{TN+TP}{TN+FP+FN+TP}
        \label{eq:svm:accuracy}
    \end{equation}
    \item{\textbf{Precision:}} How often is the classifier correct when it predicts positive.
    \begin{equation}
        \frac{TP}{TP+FP}
        \label{eq:svm:precision}
    \end{equation}
    \item{\textbf{Sensitivity (Recall):}} How often is the classifier correct when it is actually positive.
    \begin{equation}
        \frac{TP}{TP+FN}
        \label{eq:svm:sensitivity}
    \end{equation}
    \item{\textbf{\( F_1 \) Score:}} A harmonic mean of precision and sensitivity.
    \begin{equation}
        2\times\frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}
        \label{eq:svm:F1}
    \end{equation}
\end{description}

Generally, for each of these metrics, the higher value we get, the better the classification model.
