\section{Bayesian Model}
This classifier is suitable for classifying data represented by a stochastic vector. The BoVW data can be easily transformed into such vector. Instead of each component of the BoVW vector representing the number of key-points in the respective category, the component in our new vector represents the probability of key-points belonging to the respective category.

Let us denote the stochastic data vector $\Pi_{xt} \in \mathbb{R}^{K_x}, t=1,\dots,T$, where $T$ is the number of samples, $K_x$ is the size of BoVW, and. Let the vector $\Pi_{yt}\in \mathbb{R}^{K_y}$ be a vector of probabilities, with which $\Pi_{xt}$ belongs to each category, and $K_y$ the number of categories.
Let $x_t \in X \coloneqq \lbrace \mathsf{x}_1, \dots \mathsf{x}_{K_x} \rbrace, t = 1,\dots,T$ be the input variables and let $y_t \in \mathsf{Y} := \lbrace \mathsf{y}_1, \dots, \mathsf{y}_{K_Y} \rbrace$ be output categorical variables. 

Given a stochastic data vector $\Pi_x$, we can describe the transformation $\mathbb{R}^{K_x} \rightarrow \mathbb{R}^{K_y}$ using a matrix $\Delta \in \mathbb{R}^{K_y, K_x}$:
\begin{equation}
    \Delta =
    \begin{bmatrix}
        P(y_t = \mathsf{y}_1 | x_t = \mathsf{x}_1) & P(y_t = \mathsf{y}_1 | x_t = \mathsf{x}_2) & \dots & P(y_t = \mathsf{y}_1 | x_t = \mathsf{x}_{K_x})\\
        P(y_t = \mathsf{y}_2 | x_t = \mathsf{x}_1) & P(y_t = \mathsf{y}_2 | x_t = \mathsf{x}_2) & \dots & P(y_t = \mathsf{y}_2 | x_t = \mathsf{x}_{K_x})\\
        \vdots & \ddots\\
        P(y_t = \mathsf{y}_{K_y} | x_t = \mathsf{x}_1) & P(y_t = \mathsf{y}_{K_y} | x_t = \mathsf{x}_2) & \dots & P(y_t = \mathsf{y}_{K_y} | x_t = \mathsf{x}_{K_x})\\
    \end{bmatrix},
\end{equation}
where $\Pi_x^n$ is the $n$-th element of $\Pi_x$, similar to $\Pi_y^n$, and the matrix $\Delta$ is a left stochastic matrix.

The search for the optimal $\Delta^{*}$ can be written as
\begin{equation}
    \Delta^* = \argmin_{\Delta \in \Omega_{\Delta}} \sum_{t=1}^T \text{dist}(\Pi_{yt}, \Delta\Pi_{xt}),
\end{equation}
where $\Omega_{\Delta}$ is a set of left stochastic matrices. The dist($\Pi_{yt}, \Delta\Pi_{xt}$) is calculated as Kullback-Leiber divergence \cite{Kullback1951}:
\begin{equation}
    \text{dist}(\Pi_{yt}, \Delta\Pi_{xt}) = - \sum_{i=1}^{K_y} \Pi_{yt}^i \ln\frac{[\Delta\Pi_{xt}]_i}{[\Pi_{yt}]_i} = - \sum_{i=1}^{K_y} [\Pi_{yt}]_i (\ln [\Delta\Pi_{xt}]_i - \ln [\Pi_{yt}]_i).
\end{equation}
For the optimization, the term $\ln [\Pi_{yt}]_i$ is constant, therefore it can be ignored:
\begin{equation}
    \text{dist}(\Pi_{yt}, \Delta\Pi_{xt}) \propto - \sum_{i=1}^{K_y} [\Pi_{yt}]_i \ln [\Delta\Pi_{xt}]_i.
\end{equation}
The analytical solution for this problem does not exist. However, $-\ln()$ is a convex function and $\Delta\Pi_{xt}$ is a convex combination, thus Jensen's inequality can be used:
\begin{equation}
    - \sum_{i=1}^{K_y} [\Pi_{yt}]_i \ln [\Delta\Pi_{xt}]_i \leq - \sum_{i=1}^{K_y} [\Pi_{yt}]_i ( \sum_{j=1}^{K_x} [\Pi_{xt}]_j \ln (\Delta_{ij}) ) = - \sum_{i=1}^{K_y} \sum_{j=1}^{K_x} [\Pi_{yt}]_i [\Pi_{xt}]_j \ln \Delta_{ij}.
\end{equation}
From this, we get an optimization problem
\begin{equation}
    \Delta^* = \argmin_{\Delta \in \Omega_{\Delta}} - \sum_{t=1}^T \sum_{i=1}^{K_y} \sum_{j=1}^{K_x} [\Pi_{yt}]_i [\Pi_{xt}]_j \ln \Delta_{ij},
\end{equation}
where
\begin{equation}
    \Omega_{\Delta} = \{ \Delta \in [0, 1]^{K_y,K_x}, \forall j \in \{ 1, 2, \dots, K_x \} : \sum_{i=1}^{K_y} \Delta_{ij} = 1 \},
\end{equation}
which is a feasible set of left stochastic matrices. The problem can be solved analytically.

Let $\Delta$ be the optimal stochastic matrix. There exist $\lambda_j \in \mathbb{R}^{K_x}$ such that
\begin{equation}
    L(\Delta, \lambda) = - \sum_{t=1}^T \sum_{i=1}^{K_y} \sum_{j=1}^{K_x} [\Pi_{yt}]_i [\Pi_{xt}]_j \ln \Delta_{ij} + \sum_{j=1}^{K_x} \lambda_j (\sum_{i=1}^{K_y} \Delta_{ij} - 1)
\end{equation}
is the Lagrange function. The Karush-Kuhn Tucker conditions are
\begin{equation}
    \nabla_{\Delta_{\hat{i}\hat{j}}} L(\Delta_{\hat{i}\hat{j}}, \lambda) = - \frac{1}{\Delta_{\hat{i}\hat{j}}} \sum_{t=1}^{T} [\Pi_{yt}]_{\hat{i}} [\Pi_{xt}]_{\hat{j}} + \lambda_{\hat{j}} = 0,
    \label{eq:bayes_first_kkt}
\end{equation}
and
\begin{equation}
    \nabla_{\Delta_{\hat{j}}} L = \sum_{i=1}^{K_y} \Delta_{i\hat{j}} - 1 = 0.
    \label{eq:bayes_second_kkt}
\end{equation}
From \eqref{eq:bayes_first_kkt} and \eqref{eq:bayes_second_kkt}, we get the optimal $\Delta^*$ with components
\begin{equation}
    \Delta_{\hat{i}\hat{j}}^{*} = \frac{\sum_{t=1}^{T} [\Pi_{yt}]_{\hat{i}} [\Pi_{xt}]_{\hat{j}}}{\sum_{i=1}^{K_y} \sum_{t=1}^{T} [\Pi_{yt}]_{i} [\Pi_{xt}]_{\hat{j}},}.
\end{equation}

Another approach to finding the optimal $\Delta^{*}$ is optimizing the original problem
\begin{equation}
    \Delta^* = \argmin_{\Delta \in \Omega_{\Delta}} - \sum_{t=1}^T \sum_{i=1}^{K_y} [\Pi_{yt}]_i \ln [\Delta\Pi_{xt}]_i,
\end{equation}
without using the Jensen inequality. Since the feasible set $\Omega_{\Delta}$ is a closed convex set, the problem can be solved numerically using the Spectral Projected Gradient method \cite{birgin2000}.

We compare both approaches (the analytical solution using Jensen inequality and the numerical solution) in our experiments.
